{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sequence_to_Sequence_beam_search.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"WdkAyb0Rs6bW"},"source":["#**Sequence-to-sequence**"]},{"cell_type":"markdown","metadata":{"id":"yfFk6r-PujeH"},"source":["# 資料下載"]},{"cell_type":"code","metadata":{"id":"Ww4-VJoJqE-_","colab":{"base_uri":"https://localhost:8080/","height":442},"outputId":"2deb89e7-58a4-4432-bf53-e41a0b9b5289"},"source":["!gdown --id '1r4px0i-NcrnXy1-tkBsIwvYwbWnxAhcg' --output data.tar.gz\n","!tar -zxvf data.tar.gz\n","!mkdir ckpt\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading...\n","From: https://drive.google.com/uc?id=1r4px0i-NcrnXy1-tkBsIwvYwbWnxAhcg\n","To: /content/data.tar.gz\n","\r0.00B [00:00, ?B/s]\r4.72MB [00:00, 24.4MB/s]\r5.83MB [00:00, 27.3MB/s]\n","cmn-eng/\n","cmn-eng/int2word_cn.json\n","cmn-eng/int2word_en.json\n","cmn-eng/preprocess/\n","cmn-eng/preprocess/build_dataset.py\n","cmn-eng/preprocess/build_dictionary.sh\n","cmn-eng/preprocess/cmn.txt\n","cmn-eng/preprocess/cn.txt\n","cmn-eng/preprocess/dict.txt.big\n","cmn-eng/preprocess/dict.txt.small\n","cmn-eng/preprocess/en.txt\n","cmn-eng/preprocess/en_code.txt\n","cmn-eng/preprocess/en_refine.txt\n","cmn-eng/preprocess/en_vocab.txt\n","cmn-eng/preprocess/tokenizer.py\n","cmn-eng/testing.txt\n","cmn-eng/training.txt\n","cmn-eng/validation.txt\n","cmn-eng/word2int_cn.json\n","cmn-eng/word2int_en.json\n","ckpt  cmn-eng  data.tar.gz  sample_data\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BX2vLCiYuq6a"},"source":["# 下載和引入需要的 libraries"]},{"cell_type":"code","metadata":{"id":"KKShudDCiySL"},"source":["%%capture\n","!pip3 install --user nltk"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"skJCdcXBi5b5"},"source":["%%capture\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torch.utils.data as data\n","import torch.utils.data.sampler as sampler\n","import torchvision\n","from torchvision import datasets, transforms\n","\n","import numpy as np\n","import sys\n","import os\n","import random\n","import json\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # 判斷是用 CPU 還是 GPU 執行運算\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U-Irl7IluxMT"},"source":["# 資料結構"]},{"cell_type":"markdown","metadata":{"id":"VJb3zc-zwkd6"},"source":["## 定義資料的轉換\n","- 將不同長度的答案拓展到相同長度，以便訓練模型"]},{"cell_type":"code","metadata":{"id":"42Iggb94rKcI"},"source":["import numpy as np\n","# self.transform = LabelTransform(max_output_len, self.word2int_en['<PAD>'])\n","class LabelTransform(object):\n","  def __init__(self, size, pad):\n","    self.size = size\n","    self.pad = pad\n","\n","  def __call__(self, label):\n","    label = np.pad(label, (0, (self.size - label.shape[0])), mode='constant', constant_values=self.pad)\n","    return label\n","# np.pad(array, pad_width, mode, **kwargs) \n","# array 為填充目標\n","# pad_wdith 為各array需要填充的數目，此處只補後面不補前面\n","# constant 表示連續填充相同的值，constant_values為要填充的值"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8uypors6r4zD"},"source":["import re\n","import json\n","\n","class EN2CNDataset(data.Dataset):\n","  def __init__(self, root, max_output_len, set_name): \n","    # train_dataset = EN2CNDataset(config.data_path, config.max_output_len, 'training')\n","    self.root = root\n","    self.word2int_cn, self.int2word_cn = self.get_dictionary('cn')\n","    self.word2int_en, self.int2word_en = self.get_dictionary('en')\n","\n","    # 載入資料\n","    self.data = []\n","    with open(os.path.join(self.root, f'{set_name}.txt'), \"r\") as f: # f-string 可讓 set_name 直接代入\n","      for line in f:\n","        self.data.append(line)\n","    print (f'{set_name} dataset size: {len(self.data)}')\n","\n","    self.cn_vocab_size = len(self.word2int_cn)\n","    self.en_vocab_size = len(self.word2int_en)\n","    self.transform = LabelTransform(max_output_len, self.word2int_en['<PAD>'])\n","\n","  def get_dictionary(self, language):\n","    # 載入字典\n","    with open(os.path.join(self.root, f'word2int_{language}.json'), \"r\") as f:\n","      word2int = json.load(f)\n","    with open(os.path.join(self.root, f'int2word_{language}.json'), \"r\") as f:\n","      int2word = json.load(f)\n","    return word2int, int2word\n","\n","  def __len__(self):\n","    return len(self.data)\n","\n","  def __getitem__(self, Index):\n","    # 先將中英文分開\n","    sentences = self.data[Index]\n","    sentences = re.split('[\\t\\n]', sentences) ##### tab 及 換行符號，此處有待檢查\n","    sentences = list(filter(None, sentences)) ##### 此處有待檢查\n","    #print (sentences)\n","    assert len(sentences) == 2 \n","    # 確保長度為2，否則會報錯\n","\n","    # 預備特殊字元\n","    BOS = self.word2int_en['<BOS>']\n","    EOS = self.word2int_en['<EOS>']\n","    UNK = self.word2int_en['<UNK>']\n","\n","    # 在開頭添加 <BOS>，在結尾添加 <EOS> ，不在字典的 subword (詞) 用 <UNK> 取代\n","    en, cn = [BOS], [BOS]\n","    # 將句子拆解為 subword 並轉為整數\n","    sentence = re.split(' ', sentences[0])\n","    sentence = list(filter(None, sentence))\n","    #print (f'en: {sentence}')\n","    for word in sentence:\n","      en.append(self.word2int_en.get(word, UNK)) #  從字典中取出該字之編碼，若無則 return UNK = self.word2int_en['<UNK>']\n","    en.append(EOS)\n","    # 將句子拆解為單詞並轉為整數\n","    # e.g. < BOS >, we, are, friends, < EOS > --> 1, 28, 29, 205, 2\n","\n","    sentence = re.split(' ', sentences[1])\n","    sentence = list(filter(None, sentence))\n","    #print (f'cn: {sentence}')\n","    for word in sentence:\n","      cn.append(self.word2int_cn.get(word, UNK))\n","    cn.append(EOS)\n","\n","    en, cn = np.asarray(en), np.asarray(cn)\n","\n","    # 用 <PAD> 將句子補到相同長度\n","    en, cn = self.transform(en), self.transform(cn)\n","    en, cn = torch.LongTensor(en), torch.LongTensor(cn)\n","\n","    return en, cn\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WJIsNSRCyNA9"},"source":["# 模型架構"]},{"cell_type":"markdown","metadata":{"id":"3p1ZcU5HyO73"},"source":["## Encoder\n","\n"]},{"cell_type":"code","metadata":{"id":"l6uNIKqvgb9J"},"source":["class Encoder(nn.Module):\n","  def __init__(self, en_vocab_size, emb_dim, hid_dim, n_layers, dropout):\n","    super().__init__()\n","    self.embedding = nn.Embedding(en_vocab_size, emb_dim) # ex. w2vec\n","    self.hid_dim = hid_dim\n","    self.n_layers = n_layers\n","    self.rnn = nn.GRU(emb_dim, hid_dim, n_layers, dropout=dropout, batch_first=True, bidirectional=True) # RNN的變形，類似LSTM\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, input):\n","    # input = [batch size, sequence len, vocab size]\n","    embedding = self.embedding(input)\n","    outputs, hidden = self.rnn(self.dropout(embedding)) # 對 rnn 做 dropout（並非對 embedding）\n","    # outputs = [batch size, sequence len, hid dim * directions]\n","    # hidden =  [num_layers * directions, batch size  , hid dim]\n","    # outputs 是最上層RNN的輸出\n","        \n","    return outputs, hidden\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5maDs7wpyRE3"},"source":["## Decoder\n"]},{"cell_type":"code","metadata":{"id":"QTXRCkoug3ut"},"source":["class Decoder(nn.Module):\n","  def __init__(self, cn_vocab_size, emb_dim, hid_dim, n_layers, dropout, isatt):\n","    super().__init__()\n","    self.cn_vocab_size = cn_vocab_size\n","    self.hid_dim = hid_dim * 2\n","    self.n_layers = n_layers\n","    self.embedding = nn.Embedding(cn_vocab_size, config.emb_dim)\n","    self.isatt = isatt\n","    self.attention = Attention(hid_dim)\n","    # 如果使用 Attention Mechanism 會使得輸入維度變化，請在這裡修改\n","    # e.g. Attention 接在輸入後面會使得維度變化，所以輸入維度改為\n","    # self.input_dim = emb_dim + hid_dim * 2 if isatt else emb_dim\n","    self.input_dim = emb_dim\n","    self.rnn = nn.GRU(self.input_dim, self.hid_dim, self.n_layers, dropout = dropout, batch_first=True)\n","    self.embedding2vocab1 = nn.Linear(self.hid_dim, self.hid_dim * 2)\n","    self.embedding2vocab2 = nn.Linear(self.hid_dim * 2, self.hid_dim * 4)\n","    self.embedding2vocab3 = nn.Linear(self.hid_dim * 4, self.cn_vocab_size)\n","    self.dropout = nn.Dropout(dropout)\n","\n","  def forward(self, input, hidden, encoder_outputs):\n","    # input = [batch size, vocab size]\n","    # hidden = [batch size, n layers * directions, hid dim]\n","    # Decoder 只會是單向，所以 directions=1\n","    input = input.unsqueeze(1)\n","    embedded = self.dropout(self.embedding(input))\n","    # embedded = [batch size, 1, emb dim]\n","    if self.isatt:\n","      attn = self.attention(encoder_outputs, hidden)\n","      # TODO: 在這裡決定如何使用 Attention，e.g. 相加 或是 接在後面， 請注意維度變化\n","    output, hidden = self.rnn(embedded, hidden)\n","    # output = [batch size, 1, hid dim]\n","    # hidden = [num_layers, batch size, hid dim]\n","\n","    # 將 RNN 的輸出轉為每個詞出現的機率\n","    output = self.embedding2vocab1(output.squeeze(1))\n","    output = self.embedding2vocab2(output)\n","    prediction = self.embedding2vocab3(output)\n","    # prediction = [batch size, vocab size]\n","    return prediction, hidden\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"faboaaSJAcpE"},"source":["## Attention\n"]},{"cell_type":"code","metadata":{"id":"imGefKRIAfBW"},"source":["class Attention(nn.Module):\n","  def __init__(self, hid_dim):\n","    super(Attention, self).__init__()\n","    self.hid_dim = hid_dim\n","  \n","  def forward(self, encoder_outputs, decoder_hidden):\n","    # encoder_outputs = [batch size, sequence len, hid dim * directions]\n","    # decoder_hidden = [num_layers, batch size, hid dim]\n","    # 一般來說是取 Encoder 最後一層的 hidden state 來做 attention\n","    ########\n","    # TODO #\n","    ########\n","    attention=None\n","    \n","    return attention\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uJjxd56yySun"},"source":["## Seq2Seq\n"]},{"cell_type":"code","metadata":{"id":"qB-GxaQiQm5f"},"source":["# from model import Encoder, Decoder, Attention\n","import torch\n","import torch.nn as nn\n","import random\n","import torch.nn.functional as F\n","from queue import Queue\n","import operator\n","\n","class Node(object):\n","    def __init__(self, hidden, previous_node, decoder_input, attn, output, log_prob, length):\n","        self.hidden = hidden\n","        self.previous_node = previous_node\n","        self.decoder_input = decoder_input\n","        self.output = output\n","        self.attn = attn\n","        self.log_prob = log_prob\n","        self.length = length\n","\n","    def eval(self, alpha=1.0):\n","        reward = 0\n","        # Add here a function for shaping a reward\n","        #return self.log_prob\n","        return self.log_prob / float((self.length - 1)**0.7 + 1e-6) + alpha * reward"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jjko57senVKG"},"source":["class Seq2Seq(nn.Module):\n","    def __init__(self, encoder, decoder, device):\n","        super().__init__()\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.device = device\n","        assert encoder.n_layers == decoder.n_layers, \\\n","            \"Encoder and decoder must have equal number of layers!\"\n","            \n","    def forward(self, input, target, teacher_forcing_ratio):\n","        # input  = [batch size, input len, vocab size]\n","        # target = [batch size, target len, vocab size]\n","        # teacher_forcing_ratio 是有多少機率使用正確答案來訓練\n","        batch_size = target.shape[0]\n","        target_len = target.shape[1]\n","        vocab_size = self.decoder.cn_vocab_size\n","\n","        # 準備一個儲存空間來儲存輸出\n","        outputs = torch.zeros(batch_size, target_len, vocab_size).to(self.device)\n","        # 將輸入放入 Encoder\n","        encoder_outputs, hidden = self.encoder(input)\n","        # Encoder 最後的隱藏層(hidden state) 用來初始化 Decoder\n","        # encoder_outputs 主要是使用在 Attention\n","        # 因為 Encoder 是雙向的RNN，所以需要將同一層兩個方向的 hidden state 接在一起\n","        # hidden =  [num_layers * directions, batch size  , hid dim]  --> [num_layers, directions, batch size  , hid dim]\n","        hidden = hidden.view(self.encoder.n_layers, 2, batch_size, -1)\n","        hidden = torch.cat((hidden[:, -2, :, :], hidden[:, -1, :, :]), dim=2)\n","        # 取的 <BOS> token\n","        input = target[:, 0]\n","        preds = []\n","        for t in range(1, target_len):\n","            output, hidden = self.decoder(input, hidden, encoder_outputs)\n","            outputs[:, t] = output\n","            # 決定是否用正確答案來做訓練\n","            teacher_force = random.random() <= teacher_forcing_ratio\n","            # 取出機率最大的單詞\n","            top1 = output.argmax(1)\n","            # 如果是 teacher force 則用正解訓練，反之用自己預測的單詞做預測\n","            input = target[:, t] if teacher_force and t < target_len else top1\n","            preds.append(top1.unsqueeze(1))\n","        \n","        preds = torch.cat(preds, 1)\n","        return outputs, preds\n","\n","    def inference(self, input, target, beam_size):\n","        # input  = [batch size, input len, vocab size]\n","        # target = [batch size, target len, vocab size]\n","        batch_size = input.shape[0]\n","        input_len = input.shape[1]        # 取得最大字數\n","        vocab_size = self.decoder.cn_vocab_size\n","\n","        # 準備一個儲存空間來儲存輸出\n","        outputs = torch.zeros(batch_size, input_len, vocab_size).to(self.device)\n","        # 將輸入放入 Encoder\n","        encoder_outputs, hidden = self.encoder(input)\n","        # Encoder 最後的隱藏層(hidden state) 用來初始化 Decoder\n","        # encoder_outputs 主要是使用在 Attention\n","        # 因為 Encoder 是雙向的RNN，所以需要將同一層兩個方向的 hidden state 接在一起\n","        # hidden =  [num_layers * directions, batch size  , hid dim]  --> [num_layers, directions, batch size  , hid dim]\n","        hidden = hidden.view(self.encoder.n_layers, 2, batch_size, -1)\n","        hidden = torch.cat((hidden[:, -2, :, :], hidden[:, -1, :, :]), dim=2)\n","        # 取的 <BOS> token\n","        input = target[:, 0]\n","        if beam_size == 1:\n","            outputs, preds = self.greedy_decode(input, input_len, hidden, encoder_outputs, outputs)\n","        else:\n","            # Beam Search\n","            outputs, preds = self.beam_decode(input, input_len, hidden, encoder_outputs, beam_size)\n","\n","\n","        return outputs, preds\n","\n","    def greedy_decode(self, input, input_len, hidden, encoder_outputs, outputs):\n","        # beam size = 1\n","        preds = []\n","        for t in range(1, input_len):\n","            output, hidden = self.decoder(input, hidden, encoder_outputs)\n","            # 將預測結果存起來\n","            outputs[:, t] = output\n","            # 取出機率最大的單詞\n","            #top1 = output.topk(1)\n","            top1 = output.argmax(1)\n","            #print(top1.size())\n","            input = top1\n","            preds.append(top1.unsqueeze(1))\n","        \n","        preds = torch.cat(preds, 1)\n","        return outputs, preds\n","\n","    def beam_decode(self, input, input_len, hidden, encoder_outputs, beam_size):\n","        # beam search\n","        # def __init__(self, hidden, previous_node, decoder_input, attn, output, log_prob, length):\n","        root = Node(hidden, None, input, None, torch.zeros((1, 3805)).to(self.device), 0, 1)\n","        q = Queue()\n","        q.put((root.eval(), root)) # 放入\n","        end_nodes = []\n","        qsize = 1\n","        while not q.empty():\n","            candidates = []\n","            # give up when decoding takes too long\n","            # if qsize > 2000: break\n","            for _ in range(q.qsize()):\n","                score, node = q.get() # 取出\n","                input = node.decoder_input\n","                hidden = node.hidden\n","\n","                # EOS: 2\n","                print(node.decoder_input.item())\n","                print(\"111\")\n","                if node.decoder_input.item() == 2 or node.length >= 50: # 結尾了，或是句子已經太長 => 不斷從 q 取出直到 empty\n","                    end_nodes.append((node.log_prob, node))\n","                    continue\n","\n","                output, hidden = self.decoder(input, hidden, encoder_outputs)\n","                \n","                # get top k candidates at this time step\n","                log_prob, indices = F.log_softmax(output, dim=1).topk(beam_size) # 前 beam_size 個最大值\n","                print(log_prob)\n","                print(\"222\")\n","                print(indices)\n","                print(\"333\")\n","\n","                # add log probability to previous node\n","                for k in range(beam_size):\n","                    print(indices[0][k])\n","                    print(\"!!!\")\n","                    index = indices[0][k].unsqueeze(0)\n","                    print(index)\n","                    log_p = log_prob[0][k].item()\n","                    child = Node(hidden, node, index, None, output, node.log_prob + log_p, node.length + 1)\n","                    # def __init__(self, hidden, previous_node, decoder_input, attn, output, log_prob, length):\n","                    score = node.eval()\n","                    print(score)\n","                    print(\"ccc\")\n","                    candidates.append((score, child))\n","            \n","            # select top k sentence to put them in Queue\n","            candidates = sorted(candidates, key=lambda x:x[0], reverse=True)\n","            print(\"candidates:\")\n","            print(candidates)\n","            length = min(len(candidates), beam_size)\n","            for i in range(length):\n","                q.put((candidates[i][0], candidates[i][1]))\n","            \n","\n","        topk = 1 #  # how many sentence do you want to generate\n","        if len(end_nodes) == 0:\n","            end_nodes = [q.get() for _ in range(topk)]\n","\n","        utterances = []\n","        for score, n in sorted(end_nodes, key=operator.itemgetter(0), reverse=True):\n","            utterance = []\n","            utterance.append(n.decoder_input.item()) # 輸入為前一個之輸出\n","            output_prob = n.output\n","            # back trace\n","            while n.previous_node != None:\n","                n = n.previous_node\n","                output_prob = torch.cat([n.output, output_prob], dim=0)\n","                utterance.append(n.decoder_input.item())\n","            \n","            utterance = utterance[:-1]\n","            print(utterance)\n","            print(\"qqq\")\n","            utterance = utterance[::-1] # 顛倒順序\n","            print(utterance)\n","            utterances.append(utterance)\n","            break # 只有要執行一次\n","        \n","        \n","        output_prob = output_prob.unsqueeze(0)\n","        if output_prob.size()[1] != 50:\n","            output_prob = torch.cat([output_prob, torch.zeros((1, 50-output_prob.size()[1], 3805)).to(self.device)], dim=1)\n","\n","        return output_prob, utterances"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eCOIUTOT59aL"},"source":["# utils\n","\n","  "]},{"cell_type":"markdown","metadata":{"id":"vMBoCSH5MBLN"},"source":["## 儲存模型"]},{"cell_type":"code","metadata":{"id":"uCZuQrWiMGmH"},"source":["def save_model(model, optimizer, store_model_path, step):\n","  torch.save(model.state_dict(), f'{store_model_path}/model_{step}.ckpt')\n","  return"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nde98xvAMxAd"},"source":["## 載入模型"]},{"cell_type":"code","metadata":{"id":"FGzZ2Yp2MxK-"},"source":["def load_model(model, load_model_path):\n","  print(f'Load model from {load_model_path}')\n","  model.load_state_dict(torch.load(f'{load_model_path}.ckpt'))\n","  return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eoz6awEcOIAz"},"source":["## 建構模型"]},{"cell_type":"code","metadata":{"id":"TvWqv_JlOOix"},"source":["def build_model(config, en_vocab_size, cn_vocab_size):\n","  # 建構模型\n","  encoder = Encoder(en_vocab_size, config.emb_dim, config.hid_dim, config.n_layers, config.dropout)\n","  decoder = Decoder(cn_vocab_size, config.emb_dim, config.hid_dim, config.n_layers, config.dropout, config.attention)\n","  model = Seq2Seq(encoder, decoder, device)\n","  print(model)\n","  # 建構 optimizer\n","  optimizer = torch.optim.Adam(model.parameters(), lr=config.learning_rate)\n","  print(optimizer)\n","  if config.load_model:\n","    model = load_model(model, config.load_model_path)\n","  model = model.to(device)\n","\n","  return model, optimizer\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qEu4V_axXCF9"},"source":["## 數字轉句子"]},{"cell_type":"code","metadata":{"id":"ekAoI7L5mlHq"},"source":["def tokens2sentence(outputs, int2word):\n","  sentences = []\n","  for tokens in outputs:\n","    sentence = []\n","    for token in tokens:\n","      word = int2word[str(int(token))]\n","      if word == '<EOS>':\n","        break\n","      sentence.append(word)\n","    sentences.append(sentence)\n","  \n","  return sentences\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kwpb4LDFWplj"},"source":["## 計算 BLEU score"]},{"cell_type":"code","metadata":{"id":"WBRNZDsMo0hF"},"source":["import nltk\n","from nltk.translate.bleu_score import sentence_bleu\n","from nltk.translate.bleu_score import SmoothingFunction\n","\n","def computebleu(sentences, targets):\n","  score = 0 \n","  assert (len(sentences) == len(targets))\n","\n","  def cut_token(sentence):\n","    tmp = []\n","    for token in sentence:\n","      if token == '<UNK>' or token.isdigit() or len(bytes(token[0], encoding='utf-8')) == 1:\n","        tmp.append(token)\n","      else:\n","        tmp += [word for word in token]\n","    return tmp \n","\n","  for sentence, target in zip(sentences, targets):\n","    sentence = cut_token(sentence)\n","    target = cut_token(target)\n","    score += sentence_bleu([target], sentence, weights=(1, 0, 0, 0))                                                                                          \n","  \n","  return score\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-vijuXbMeJrn"},"source":["##迭代 dataloader"]},{"cell_type":"code","metadata":{"id":"iIZ44EdfeJ3i"},"source":["def infinite_iter(data_loader):\n","  it = iter(data_loader)\n","  while True:\n","    try:\n","      ret = next(it)\n","      yield ret\n","    except StopIteration:\n","      it = iter(data_loader)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aIfx7oj5fAjP"},"source":["## schedule_sampling"]},{"cell_type":"code","metadata":{"id":"AHuxM8m-fArz"},"source":["########\n","# TODO #\n","########\n","\n","\n","def schedule_sampling():\n","    return 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bBHAhX5e5xm4"},"source":["# 訓練步驟"]},{"cell_type":"markdown","metadata":{"id":"_q9Co3vuGWfu"},"source":["## 訓練\n","- 訓練階段"]},{"cell_type":"code","metadata":{"id":"TBnE9GbiO8Ob"},"source":["def train(model, optimizer, train_iter, loss_function, total_steps, summary_steps, train_dataset):\n","  model.train()\n","  model.zero_grad()\n","  losses = []\n","  loss_sum = 0.0\n","  for step in range(summary_steps):\n","    sources, targets = next(train_iter)\n","    sources, targets = sources.to(device), targets.to(device)\n","    outputs, preds = model(sources, targets, schedule_sampling())\n","    # targets 的第一個 token 是 <BOS> 所以忽略\n","    outputs = outputs[:, 1:].reshape(-1, outputs.size(2))\n","    targets = targets[:, 1:].reshape(-1)\n","    loss = loss_function(outputs, targets)\n","    \n","    optimizer.zero_grad()\n","    loss.backward()\n","    grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n","    optimizer.step()\n","\n","    loss_sum += loss.item()\n","    if (step + 1) % 5 == 0:\n","      loss_sum = loss_sum / 5\n","      print (\"\\r\", \"train [{}] loss: {:.3f}, Perplexity: {:.3f}      \".format(total_steps + step + 1, loss_sum, np.exp(loss_sum)), end=\" \")\n","      losses.append(loss_sum)\n","      loss_sum = 0.0\n","\n","  return model, optimizer, losses\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6cuHVXxAfHrA"},"source":["## 檢驗/測試\n","- 防止訓練發生overfitting"]},{"cell_type":"code","metadata":{"id":"ZBp-n3FrfOCe"},"source":["def test(model, dataloader, loss_function):\n","  model.eval()\n","  loss_sum, bleu_score= 0.0, 0.0\n","  n = 0\n","  result = []\n","  for sources, targets in dataloader:\n","    sources, targets = sources.to(device), targets.to(device)\n","    batch_size = sources.size(0)\n","    outputs, preds = model.inference(sources, targets, 5) # 設定 beam_size\n","    # targets 的第一個 token 是 <BOS> 所以忽略\n","    outputs = outputs[:, 1:].reshape(-1, outputs.size(2))\n","    targets = targets[:, 1:].reshape(-1)\n","\n","    loss = loss_function(outputs, targets)\n","    loss_sum += loss.item()\n","\n","    # 將預測結果轉為文字\n","    targets = targets.view(sources.size(0), -1)\n","    preds = tokens2sentence(preds, dataloader.dataset.int2word_cn)\n","    sources = tokens2sentence(sources, dataloader.dataset.int2word_en)\n","    targets = tokens2sentence(targets, dataloader.dataset.int2word_cn)\n","    for source, pred, target in zip(sources, preds, targets):\n","      result.append((source, pred, target))\n","    # 計算 Bleu Score\n","    bleu_score += computebleu(preds, targets)\n","\n","    n += batch_size\n","\n","  return loss_sum / len(dataloader), bleu_score / n, result\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dFEYmPAlx_SX"},"source":["## 訓練流程\n","- 先訓練，再檢驗"]},{"cell_type":"code","metadata":{"id":"cJ54vDP2yC2S"},"source":["def train_process(config):\n","  # 準備訓練資料\n","  train_dataset = EN2CNDataset(config.data_path, config.max_output_len, 'training')\n","  train_loader = data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)\n","  train_iter = infinite_iter(train_loader)\n","  # 準備檢驗資料\n","  val_dataset = EN2CNDataset(config.data_path, config.max_output_len, 'validation')\n","  val_loader = data.DataLoader(val_dataset, batch_size=1)\n","  # 建構模型\n","  model, optimizer = build_model(config, train_dataset.en_vocab_size, train_dataset.cn_vocab_size)\n","  loss_function = nn.CrossEntropyLoss(ignore_index=0)\n","\n","  train_losses, val_losses, bleu_scores = [], [], []\n","  total_steps = 0\n","  while (total_steps < config.num_steps):\n","    # 訓練模型\n","    model, optimizer, loss = train(model, optimizer, train_iter, loss_function, total_steps, config.summary_steps, train_dataset)\n","    train_losses += loss\n","    # 檢驗模型\n","    val_loss, bleu_score, result = test(model, val_loader, loss_function)\n","    val_losses.append(val_loss)\n","    bleu_scores.append(bleu_score)\n","\n","    total_steps += config.summary_steps\n","    print (\"\\r\", \"val [{}] loss: {:.3f}, Perplexity: {:.3f}, blue score: {:.3f}       \".format(total_steps, val_loss, np.exp(val_loss), bleu_score))\n","    \n","    # 儲存模型和結果\n","    if total_steps % config.store_steps == 0 or total_steps >= config.num_steps:\n","      save_model(model, optimizer, config.store_model_path, total_steps)\n","      with open(f'{config.store_model_path}/output_{total_steps}.txt', 'w') as f:\n","        for line in result:\n","          print (line, file=f)\n","    \n","  return train_losses, val_losses, bleu_scores\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QZvo0u9SofwW"},"source":["## 測試流程"]},{"cell_type":"code","metadata":{"id":"tvpiHCM-ogNh"},"source":["def test_process(config):\n","  # 準備測試資料\n","  test_dataset = EN2CNDataset(config.data_path, config.max_output_len, 'testing')\n","  test_loader = data.DataLoader(test_dataset, batch_size=1)\n","  # 建構模型\n","  model, optimizer = build_model(config, test_dataset.en_vocab_size, test_dataset.cn_vocab_size)\n","  print (\"Finish build model\")\n","  loss_function = nn.CrossEntropyLoss(ignore_index=0)\n","  model.eval()\n","  # 測試模型\n","  test_loss, bleu_score, result = test(model, test_loader, loss_function)\n","  # 儲存結果\n","  with open(f'./test_output.txt', 'w') as f:\n","    for line in result:\n","      print (line, file=f)\n","\n","  return test_loss, bleu_score\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7PbfgB3n9eoT"},"source":["# Config\n","- 實驗的參數設定表"]},{"cell_type":"code","metadata":{"id":"3kWSZ4w39gzj"},"source":["class configurations(object):\n","  def __init__(self):\n","    self.batch_size = 60\n","    self.emb_dim = 256\n","    self.hid_dim = 512\n","    self.n_layers = 3\n","    self.dropout = 0.5\n","    self.learning_rate = 0.00005\n","    self.max_output_len = 50              # 最後輸出句子的最大長度\n","    self.num_steps = 12000                # 總訓練次數\n","    self.store_steps = 300                # 訓練多少次後須儲存模型\n","    self.summary_steps = 300              # 訓練多少次後須檢驗是否有overfitting\n","    self.load_model = False               # 是否需載入模型\n","    self.store_model_path = \"./ckpt\"      # 儲存模型的位置\n","    self.load_model_path = None           # 載入模型的位置 e.g. \"./ckpt/model_{step}\" \n","    self.data_path = \"./cmn-eng\"          # 資料存放的位置\n","    self.attention = False                # 是否使用 Attention Mechanism\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w464f4KOLUh6"},"source":["#Main Function\n","- 讀入參數\n","- 進行訓練或是推論"]},{"cell_type":"markdown","metadata":{"id":"qlwNx0z6vu_S"},"source":["## train"]},{"cell_type":"code","metadata":{"id":"AJwVkorvLaSh"},"source":["if __name__ == '__main__':\n","  config = configurations()\n","  print ('config:\\n', vars(config))\n","  train_losses, val_losses, bleu_scores = train_process(config)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2ig1XJkEKHSF"},"source":["class configurations(object):\n","  def __init__(self):\n","    self.batch_size = 60\n","    self.emb_dim = 256\n","    self.hid_dim = 512\n","    self.n_layers = 3\n","    self.dropout = 0.5\n","    self.learning_rate = 0.00005\n","    self.max_output_len = 50              # 最後輸出句子的最大長度\n","    self.num_steps = 12000                # 總訓練次數\n","    self.store_steps = 300                # 訓練多少次後須儲存模型\n","    self.summary_steps = 300              # 訓練多少次後須檢驗是否有overfitting\n","    self.load_model = True             # 是否需載入模型\n","    self.store_model_path = \"./ckpt\"      # 儲存模型的位置\n","    self.load_model_path = \"./ckpt/model_3300\"           # 載入模型的位置 e.g. \"./ckpt/model_{step}\" \n","    self.data_path = \"./cmn-eng\"          # 資料存放的位置\n","    self.attention = False                # 是否使用 Attention Mechanism"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EjDNDiwZvyCn"},"source":["## test"]},{"cell_type":"code","metadata":{"id":"N00ZAkL7ieGP"},"source":["# 在執行 Test 之前，請先行至 config 設定所要載入的模型位置\n","if __name__ == '__main__':\n","  config = configurations()\n","  print ('config:\\n', vars(config))\n","  test_loss, bleu_score = test_process(config)\n","  print (f'test loss: {test_loss}, bleu_score: {bleu_score}')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JyhX44s2wS1I"},"source":["# 圖形化訓練過程"]},{"cell_type":"markdown","metadata":{"id":"nNnx0AcDxiJz"},"source":["## 以圖表呈現 訓練 的 loss 變化趨勢"]},{"cell_type":"code","metadata":{"id":"EahLuZ2X8zdF"},"source":["import matplotlib.pyplot as plt\n","plt.figure()\n","plt.plot(train_losses)\n","plt.xlabel('次數')\n","plt.ylabel('loss')\n","plt.title('train loss')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WbEi0MMGxl-v"},"source":["## 以圖表呈現 檢驗 的 loss 變化趨勢"]},{"cell_type":"code","metadata":{"id":"tJEqy0aBxA3E"},"source":["import matplotlib.pyplot as plt\n","plt.figure()\n","plt.plot(val_losses)\n","plt.xlabel('次數')\n","plt.ylabel('loss')\n","plt.title('validation loss')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C5SJkPbgxp0m"},"source":["## BLEU score"]},{"cell_type":"code","metadata":{"id":"jhQ3Z7NPxBB8"},"source":["import matplotlib.pyplot as plt\n","plt.figure()\n","plt.plot(bleu_scores)\n","plt.xlabel('次數')\n","plt.ylabel('BLEU score')\n","plt.title('BLEU score')\n","plt.show()"],"execution_count":null,"outputs":[]}]}